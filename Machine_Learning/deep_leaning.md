# 深度学习
## 基础知识
### 激活函数
帮网络引入“非线性”。能使神经网络学会模式和规律。否则线性关系的神经网络实在太差。
常见激活函数：
1. Sigmoid
  - 二分类，分为0和1
  - 可能导致梯度消失
2. Relu
  - 小于0输出0，大于0输出不变
  - 缓解梯度消失（函数导数：x<0 时恒等于0，大于等于0时为1，也就是说此时Relu的梯度恒等1，不会变小不会消失。反之出现神经元失活）
3. Tanh
  - 把数字归一到-1到1之间
4. Leaky Relu
  - 负数区域允许小斜率通过
  - 改善ReLU“死亡神经元”问题（死亡神经元：输入为负，梯度恒等于0，永远无法更新）

### BN 批量归一化
全称：Batch Normalization  
可以通过规整数据的分布基本解决梯度消失/爆炸的问题（避免输入值过大或过小），但在RNN或是极深的网络中不能完全解决，还是需要激活函数、残差连接等。  
简单来讲，就是使神经网络中的数据保持稳定。  
神经网络在训练时，每一层输入的数据分布会不断变化（叫“内部协变量偏移”），导致训练变慢、不稳定。BN把每一层输入的数据“调成”均值为0、方差为1的标准状态，自动调整每一层的输入，让网络训练更快更稳定。

1. 把当前 mini-batch （一小批训练样本）里的数据算均值和方差。
2. 用这个均值和方差把数据“归一化”，变成平均值为0，标准差为1。
3. 加上两个可学习的参数（缩放因子γ和平移因子β），让网络自己决定是否保留归一化结果或者做调整。

一般在Relu激活函数的前面使用。


## 常见架构
- CNN（以下均为局部感受野（卷积）作为核心特征提取手段，且均为浅层到深层）
  - ResNet
  - Yolo
  -（V-Net）
- Transformer
  - 基于自注意力（Self-Attention）机制
  - BERT
  - GPT
  - ViT
- RNN
- MLP  

## CNN
核心：卷积、池化  
擅长做图像、网格数据处理  
复杂度：O(n)局部感受野  
部分并行（依赖卷积核大小）	

可以把整张图像的卷积输出的特征想象成特征池（标准来讲是多维特征图），突出的显著特征会高一些。卷积操作通过滑动窗口（卷积核）对图像进行局部扫描，提取不同区域的空间特征。对卷积得到的特征使用激活函数进行激活，可以解决非线性问题。（由于卷积操作是由输入矩阵与卷积核矩阵进行相差的线性变化关系，需要激活层对其进行非线性的映射。）对特征池中的所有特征进行池化，缩小池中的信息，保留最显著的特征。池子可能不止一个，有浅层和深层，浅层可能为边缘，深层可能为物体检测。所以浅层是对图像进行初步的边缘、颜色、纹理，深层是学习如何组合浅层输出的特征，如语义。  
**实际代码中，这些“池子”就是多维张量（如[batch, channels, height, width]）**

### Yolo 目标检测 2D
Backbone (CNN) → Neck (FPN/PAN) → Head (分类+回归)  
实时的目标检测算法  
YOLO 会将整个图像划分为一个 S×S 网格，每个网格负责预测一定数量的目标（v1-v5），或者像素点中心（v8）
- 坐标（x, y, w, h）
- 置信度
- 类别概率向量:“在当前位置有目标”的前提下，该目标属于每个类别的概率
最终输出是一个形如 [N, 6] 的张量，N 为检测到的目标数，6 包括 [x, y, w, h, confidence, class]
YOLO 的输出是密集预测的框集合，它并不会尝试把多个相邻网格的目标“拼接”起来。最后会通过非极大值抑制来筛选掉重复的检测框，留下可能性最高，最可靠的检测框。

检测过程：
1. 输入一张猫猫图，640*640*3（RGB）
2. CNN，卷它，拿特征
3. 然后通过激活函数，使它非线性化。通过池化层，对特征进行降采样。
4. 现在它是一个有明显特征的小尺寸的复杂通道猫猫（80*80*256）
5. Neck层，v8中使用PAN。让网络同时关注大目标（大区域）和小目标（细节）
6. HEAD输出预测，每个位置都做一个检测框。假设我现在是（80*80），那就有6400个位置，每个位置可以预测多个检测框：[x, y, w, h, objectness, class_probs...]（坐标，置信度，类别概率）
7. 后处理，非极大值抑制NMS。框太多了，我要找到最猫猫的猫猫。筛选掉低分数的猫猫框，将框和分数最高的那个一一做对比（有一个阈值），小于的放一边（放下一轮去看，万一不止一只猫猫），大于阈值的，就是框重叠了，合并成分数最高的那个
8. 


### ResNet 残差神经网络 2D
x → [Conv1x1, Conv3x3(groups=32), Conv1x1] + x → ReLU  
在ResNet网络提出之前，传统的卷积神经网络都是通过将一系列卷积层与下采样层进行堆叠得到的。但是当堆叠到一定网络深度时，就会出现两个问题：
- 梯度消失或梯度爆炸
  - 梯度消失/爆炸：神经网络在**反向传播**的时候，反向连乘的梯度小于1（或大于1），导致连乘的次数多了之后（网络层数加深），传回首层的梯度过小甚至为0（过大甚至无穷大）
- 退化问题(degradation problem)
  - 网络越深，优化越困难，导致训练误差上升（即退化。不属于过拟合，而是模型本身难以学习。
- 其它扩展随便写这里了，不属于这里：过拟合
  - 过拟合：网络在训练集上表现很好，在验证集上表现差

梯度爆炸可以通过BN来解决，ResNet可以解决网络的退化问题。

普通的神经网络如下：
<img width="232" height="429" alt="image" src="https://github.com/user-attachments/assets/9deac40b-d788-476a-8d9f-86414ba05fd6" />  
残差神经网络如下:
<img width="1189" height="650" alt="image" src="https://github.com/user-attachments/assets/ac23bd27-dcab-4c35-819e-3beb7ee90bf0" />  
可以简单理解为：传统的神经网络是构建了一个完整的函数，这种方法会造成信息丢书。而残差神经网络是只学习差值，更加稳定可控。学习的是 F(x)，不是 H(x)。
H(x)对于模型来说可以理解为从头学起，全部依靠网络学习。而F(x)可以理解为模型的默认输出就是x，F(x)只是一个微调的值。


### V-Net 医学分割 3D
U-Net 2D: [Downsample → 2D Conv] ×4 → [Upsample + Skip] ×4  
V-Net 3D: [Downsample → 3D Conv] ×4 → [Upsample + 3D Skip] ×4  
编码器（下采样）  
1. 使用多个3D卷积 + Relu。
2. 使用stride=2的卷积代替了最大池化的下采样。
3. 提取低分辨率高语义的体积特征（利用深度信息，学习的还是语义）
解码器（上采样）
1. 使用转置卷积进行上采样（deconv）
2. 每一步 upsample 都和对称的 encoder 层拼接（skip connection），类似ResNet

特殊：使用 Dice 损失函数（Dice Loss），比交叉熵更适合处理类别不平衡问题，提升小目标表现


### CNN架构对比表（YOLO vs ResNet vs V-Net）

| 对比维度         | YOLO (以YOLOv8为例)                          | ResNet (以ResNet50为例)                  | V-Net                              |
|------------------|---------------------------------------------|-----------------------------------------|-----------------------------------|
| **核心结构**     | CSPDarknet + PANet + 检测头                 | 残差块 (Bottleneck) + 全局平均池化       | 3D U-Net结构 + 跳跃连接            |
| **输入维度**     | 2D RGB图像 (e.g., 640×640×3)                | 2D RGB图像 (e.g., 224×224×3)            | 3D体数据 (e.g., 128×128×128×1)    |
| **主要任务**     | 实时目标检测 (分类+定位)                    | 图像分类                                | 医学影像分割                      |
| **特征提取差异** | 多尺度特征融合 (FPN/PANet)                  | 深层语义特征 (通过残差连接保留梯度)       | 3D空间上下文特征 (体素级分析)      |
| **核心创新点**   | 1. 单阶段检测<br>2. Anchor-Free设计         | 1. 残差学习<br>2. 恒等映射              | 1. 3D卷积<br>2. Dice Loss         |
| **输出形式**     | 边界框 (x,y,w,h) + 类别概率                 | 类别概率向量                            | 3D分割掩码 (与输入同尺寸)          |
| **典型应用**     | 自动驾驶、视频监控                          | ImageNet分类、迁移学习                  | 肿瘤分割、器官三维重建             |
| **计算复杂度**   | 中 (约50 GFLOPs @640×640)                  | 高 (约4 GFLOPs @224×224)               | 极高 (约500 GFLOPs @128³)         |
| **开源实现**     | [Ultralytics](https://github.com/ultralytics) | [TorchVision](https://pytorch.org/vision) | [MONAI](https://monai.io/)       |  

  
## Transformer
核心：	自注意力、前馈网络
擅长：文本、序列数据，但现在也可用于图像  
O(n^2)全局注意力  
训练阶段可以完全并行，但在语言生成的推理阶段仍然为顺序执行  

Transformer（ViT）：将图像分块为序列，用Transformer处理，挑战CNN的视觉霸权  

可以将transformer理解为动态特征池。当输入图像时，图像会被拆解成一个个拼图块，排列成序列，其中每一个图像块转化为特征向量。使用自注意力机制，每个拼图块自主寻找其他相关块（如猫猫头找猫猫身），通过加权求和重构自身特征（找更多的猫猫特征）。重组后的拼图块被分类头解码为最终结果。（学习到图像中有猫猫）  

CNN的“池”是局部固定的（卷积核尺寸限制），靠深度堆叠扩大感受野。
Transformer的“池”是全局动态的，通过注意力权重直接关联任意距离的元素。 

1.  **Self-Attention 自注意力**：输入一个序列，输出一个序列，长度相同。每个位置都需要计算注意力，允许模型理解上下文关系。：QWE和EWQ
2.  **Multi-Head Attention 多头注意力**：拆分，分别进行学习：1关注“语义”，2关注“词性”，3关注“句法”
3.  **Cross-Attention 交叉注意力**：在序列中解码当前词，到另一个序列中编码输出：常用于翻译模型，输入ich，解码器输出ich，到编码器中查找我，输出翻译
4.  **Sparse Attention 稀疏注意力**：每个位置只关注部分重要位置，降低计算复杂度
5.  **Local Attention 局部注意力**：每个位置只关注临近的区域，如相邻四位
6.  **Global Attention 全局注意力**：每个位置对所有位置都关注，精度高，开销大

### Vit 图像分类
全称：Vision Transformer
1. 将图像分成指定大小的非重叠像素块，展平排列为向量序列
2. 每一个图像像素块都有一个可学习的位置信息，用来弥补对空间感知的缺陷
3. 通过自注意力机制建模图像中各区域之间的关系，最后输出图像分类或分割结果
4. 结果序列开头加一个 class

**大小一般为16*16、8*8、32*32、14*14**，16×16（性能与计算量的平衡点），资源受限：32×32（速度优先），高精度需求：8×8（需配合混合精度训练）。**ImageNet标准输入尺寸为224×224**

详细举例：
1. 输入图像 224*224*3（RGB、BGR无具体要求）
2. patch 大小：16*16
3. 得到 14*14 = 196 个 patch，加通道数一起算就是 16*16*3 = 768 维的向量序列
4. 加上位置信息，第一个 0，作为可学习的位置编码
5. 在序列前插入一个特殊向量[CLS]作为类别
6. 最终，输入序列长度变为197：196 patch + 1CLS。维度不变，768。整个序列的shape：[1,197,768]
7. 将整个patch序列喂给transformer编码器（比如BERT），编码器有多层，每一层编码器都做同样的事情：Multi-head Self-Attention → 残差连接 → LayerNorm → MLP → 残差连接 → LayerNorm
8. 在每一层的自注意力机制中，cls会获取其它patch的信息，模型会理解上下文的关系。
9. [CLS] token 的向量就成了一个图像全局语义的表达向量，它看过且学习了全图所有区域的信息

图像 → Patch → [CLS] + Patch序列 → Transformer 编码器 → 得到 [CLS] 向量  → 全连接层 → 分类 logits → softmax → 各类别概率



## 常用激活函数和架构使用组合
如果没有激活函数，神经网络无论有多少层，都只能表示线性变换（多个线性层的叠加仍是线性的），无法解决非线性问题（如异或分类、图像识别等）。
激活函数（如ReLU、Sigmoid）可以对输入进行非线性变换，使神经网络可以拟合任意复杂的函数。没有非线性，神经网络就失去了解决复杂问题的能力。  
- CNN中ReLU能帮助模型捕捉图像的层次化特征。


## 参考内容
https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch03_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%B8%89%E7%AB%A0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md  

https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/%E7%AC%AC%E4%BA%94%E7%AB%A0_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN).md  
