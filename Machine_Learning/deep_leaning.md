# 激活函数
  如果没有激活函数，神经网络无论有多少层，都只能表示线性变换（多个线性层的叠加仍是线性的），无法解决非线性问题（如异或分类、图像识别等）。
  激活函数（如ReLU、Sigmoid）可以对输入进行非线性变换，使神经网络可以拟合任意复杂的函数。没有非线性，神经网络就失去了解决复杂问题的能力。  

## 常见架构
- CNN
  - ResNet
  - Yolo
  -（V-Net）
- Transformer
  - 基于自注意力（Self-Attention）机制
  - BERT
  - GPT
  - ViT
- RNN
- MLP  

### CNN

### Transformer

## 常用激活函数和架构使用组合
- CNN中ReLU能帮助模型捕捉图像的层次化特征。
